{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0baef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from ai.h2o.sparkling.H2OContext import H2OContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405c66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"H2O\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90d6ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://ajays-air:54325 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>20 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Los_Angeles</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.32.1.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>2 months and 2 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>sparkling-water-aj_local-1626938892146</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>574 Mb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://ajays-air:54325</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>null</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>XGBoost, Algos, Amazon S3, Sparkling Water REST API Extensions, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.9.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------------------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         20 secs\n",
       "H2O_cluster_timezone:       America/Los_Angeles\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.32.1.3\n",
       "H2O_cluster_version_age:    2 months and 2 days\n",
       "H2O_cluster_name:           sparkling-water-aj_local-1626938892146\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    574 Mb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://ajays-air:54325\n",
       "H2O_connection_proxy:       null\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         XGBoost, Algos, Amazon S3, Sparkling Water REST API Extensions, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.9.5 final\n",
       "--------------------------  -------------------------------------------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * Sparkling Water Version: 3.32.1.3-1-3.0\n",
      " * H2O name: sparkling-water-aj_local-1626938892146\n",
      " * cluster size: 1\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (0,192.168.43.37,54323)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://ajays-air:54325 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "hc = H2OContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d082c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateScore(prob):\n",
    "    \"\"\"\n",
    "\n",
    "    :param prob: probability of model on data\n",
    "    :return: it's return score\n",
    "    \"\"\"\n",
    "    pdo = 20\n",
    "    odds = prob / (1 - prob)\n",
    "    factor = pdo / np.log(2)\n",
    "    offset = 500 - factor * np.log(pdo)\n",
    "    score = offset + factor * np.log(odds)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4707161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilityHelperPSI_ModelComparison(initial, new):\n",
    "    \"\"\"\n",
    "\n",
    "    :param initial:\n",
    "    :param new:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    size = 11\n",
    "    mini = min(initial)\n",
    "    maxi = max(new)\n",
    "    add = (maxi - mini) / size\n",
    "\n",
    "    binSupporter = [mini if (i == 0) else (mini := mini + add) for i in range(size)]\n",
    "    initial_counts = np.histogram(initial, binSupporter)[0]\n",
    "    new_counts = np.histogram(new, binSupporter)[0]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {'Bucket': (np.arange(1, size)), 'Breakpoint Value': binSupporter[1:], 'Train Count': initial_counts,\n",
    "         'Test Count': new_counts})\n",
    "    df['Train Percent'] = df['Train Count'] / len(initial)\n",
    "    df['Test Percent'] = df['Test Count'] / len(new)\n",
    "    df['Test Percent'][df['Test Percent'] == 0] = 0.001\n",
    "    df['Train Percent'][df['Train Percent'] == 0] = 0.001\n",
    "\n",
    "    score = pd.DataFrame()\n",
    "    score['Score'] = (df['Test Percent'] - df['Train Percent']) * np.log(df['Test Percent'] / df['Train Percent'])\n",
    "    score.loc[len(score.index)] = np.sum(score['Score'])\n",
    "\n",
    "    df = pd.concat([df, score], axis=1, sort=False)\n",
    "    df = df.replace(np.nan, \"\", regex=True)\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    return df.round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dffc46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stabilityHelperPSI_OngoingComparison(initial, new):\n",
    "    \"\"\"\n",
    "\n",
    "    :param initial:\n",
    "    :param new:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    size = 11\n",
    "    mini = min(initial)\n",
    "    maxi = max(new)\n",
    "    add = (maxi - mini) / size\n",
    "\n",
    "    binSupporter = [mini if (i == 0) else (mini := mini + add) for i in range(size)]\n",
    "    initial_counts = np.histogram(initial, binSupporter)[0]\n",
    "    new_counts = np.histogram(new, binSupporter)[0]\n",
    "\n",
    "    df = pd.DataFrame({'Bucket': np.arange(1, size),\n",
    "                       'Breakpoint Value': binSupporter[1:],\n",
    "                       'Development Count': initial_counts,\n",
    "                       'Monitoring Count': new_counts})\n",
    "    df['Development Percent'] = df['Development Count'] / len(initial)\n",
    "    df['Monitoring Percent'] = df['Monitoring Count'] / len(new)\n",
    "    df['Monitoring Percent'][df['Monitoring Percent'] == 0] = 0.001\n",
    "    df['Development Percent'][df['Development Percent'] == 0] = 0.001\n",
    "\n",
    "    score = pd.DataFrame()\n",
    "    score['Score'] = (df['Monitoring Percent'] - df['Development Percent']) * np.log(\n",
    "        df['Monitoring Percent'] / df['Development Percent'])\n",
    "    score.loc[len(score.index)] = np.sum(score['Score'])\n",
    "\n",
    "    df = pd.concat([df, score], axis=1, sort=False)\n",
    "    df = df.replace(np.nan, \"\", regex=True)\n",
    "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    return df.round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18694798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSI_OngoingComparison(developmentProb, monitoringProb):\n",
    "    \"\"\"\n",
    "\n",
    "    :param developmentProb:\n",
    "    :param monitoringProb:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    developmentScore = generateScore(developmentProb)\n",
    "    monitoringScore = generateScore(monitoringProb)\n",
    "    psi_table = stabilityHelperPSI_OngoingComparison(developmentScore, monitoringScore)\n",
    "    return psi_table\n",
    "\n",
    "\n",
    "def PSI_ModelComparison(developmentProb, monitoringProb):\n",
    "    \"\"\"\n",
    "\n",
    "    :param developmentProb:\n",
    "    :param monitoringProb:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    developmentScore = generateScore(developmentProb)\n",
    "    monitoringScore = generateScore(monitoringProb)\n",
    "    psi_table = stabilityHelperPSI_ModelComparison(developmentScore, monitoringScore)\n",
    "    return psi_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9018f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSI TEST\n",
    "def stabilityHelper(initial, new):\n",
    "    \"\"\"\n",
    "\n",
    "    :param initial:\n",
    "    :param new:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    size = 11\n",
    "    mini = min(initial)\n",
    "    maxi = max(new)\n",
    "    add = (maxi - mini) / size\n",
    "\n",
    "    binSupporter = [mini if (i == 0) else (mini := mini + add) for i in range(size)]\n",
    "    initial_counts = np.histogram(initial, binSupporter)[0]\n",
    "    new_counts = np.histogram(new, binSupporter)[0]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {'Bucket': np.arange(1, size), 'Breakpoint Value': binSupporter[1:], 'Initial Count': initial_counts,\n",
    "         'New Count': new_counts})\n",
    "    df['Initial Percent'] = df['Initial Count'] / len(initial)\n",
    "    df['New Percent'] = df['New Count'] / len(new)\n",
    "    df['New Percent'][df['New Percent'] == 0] = 0.001\n",
    "    df['Initial Percent'][df['Initial Percent'] == 0] = 0.001\n",
    "    df['score'] = (df['New Percent'] - df['Initial Percent']) * np.log(df['New Percent'] / df['Initial Percent'])\n",
    "    score = np.sum(df['score'])\n",
    "    score = np.round(score, 4)\n",
    "    return score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd7fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CSI(development, monitoring):\n",
    "    \"\"\"\n",
    "\n",
    "    :param development:\n",
    "    :param monitoring:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    csi = stabilityHelper(development, monitoring)\n",
    "    return csi\n",
    "\n",
    "\n",
    "def calc_csi(column_name, X_train, X_test):\n",
    "    \"\"\"\n",
    "\n",
    "    :param column_name:\n",
    "    :param X_train:\n",
    "    :param X_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return CSI(X_train[column_name].values, X_test[column_name].values)\n",
    "\n",
    "\n",
    "def calculate_csi(x_test, x_train):\n",
    "    \"\"\"\n",
    "\n",
    "    :param x_test:\n",
    "    :param x_train:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    columns = x_test.columns.to_list()\n",
    "    csi_dict = {}\n",
    "    for col in columns:\n",
    "        test = calc_csi(col, x_train, x_test)\n",
    "        csi_dict[col] = test\n",
    "\n",
    "    table = pd.DataFrame({'Variable': list(csi_dict.keys()), 'CSI Score': list(csi_dict.values())}).round(2)\n",
    "    return table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59fc51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def KS_OngoingComparison(user, y_probas, y_true, x_probas, x_true):\n",
    "    \"\"\"\n",
    "\n",
    "    :param user:\n",
    "    :param y_probas:\n",
    "    :param y_true:\n",
    "    :param x_probas:\n",
    "    :param x_true:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    helper = Model_helper(username=user)\n",
    "    colors = helper.preference_maker(2)\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    y_probas = y_probas.ravel()\n",
    "    thresholds, pct1, pct2, ks_statistic, max_distance_at, classes = binary_ks_curve(y_true, y_probas)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    idx = np.where(thresholds == max_distance_at)[0][0]\n",
    "    fig.add_shape(dict(type=\"line\", x0=max_distance_at, y0=pct1[idx], x1=max_distance_at, y1=pct2[idx],\n",
    "                       line=dict(color=\"rgba(0,0,0,0)\", width=3, dash=\"dot\")))\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=pct2, mode='lines', name='Class 1', line=dict(color=colors[0])))\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=pct1, mode='lines', name='Class 0', line=dict(color=colors[1])))\n",
    "    fig.add_annotation(x=1, y=0.05, showarrow=False,\n",
    "                       text='KS Statistic : {:.3f} at {:.3f}'.format(ks_statistic, max_distance_at))\n",
    "    fig.update_layout(title_text=f\"Monitoring KS Statistic\", title_x=0.5)\n",
    "    fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig.update_xaxes(title_text='Threshold')\n",
    "    fig.update_yaxes(title_text='Percentage Below Threshold')\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.23, xanchor=\"right\", x=0.6))\n",
    "\n",
    "    x_true = np.array(x_true)\n",
    "    x_probas = np.array(x_probas).ravel()\n",
    "    thresholds, pct1, pct2, ks_statistic, max_distance_at, classes = binary_ks_curve(x_true, x_probas)\n",
    "\n",
    "    fig2 = go.Figure()\n",
    "    idx = np.where(thresholds == max_distance_at)[0][0]\n",
    "    fig2.add_shape(dict(type=\"line\", x0=max_distance_at, y0=pct1[idx], x1=max_distance_at, y1=pct2[idx],\n",
    "                        line=dict(color=\"rgba(0,0,0,0)\", width=3, dash=\"dot\")))\n",
    "    fig2.add_trace(go.Scatter(x=thresholds, y=pct2, mode='lines', name='Class 1', line=dict(color=colors[0])))\n",
    "    fig2.add_trace(go.Scatter(x=thresholds, y=pct1, mode='lines', name='Class 0', line=dict(color=colors[1])))\n",
    "    fig2.add_annotation(x=1, y=0.05, showarrow=False,\n",
    "                        text='KS Statistic : {:.3f} at {:.3f}'.format(ks_statistic, max_distance_at))\n",
    "    fig2.update_layout(title_text=f\"Development KS Statistic\", title_x=0.5)\n",
    "    fig2.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig2.update_xaxes(title_text='Threshold')\n",
    "    fig2.update_yaxes(title_text='Percentage Below Threshold')\n",
    "    fig2.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig2.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig2.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.23, xanchor=\"right\", x=0.6))\n",
    "    return fig, fig2\n",
    "\n",
    "\n",
    "def KS_ModelComparison(y_probas, y_true, user):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_probas:\n",
    "    :param y_true:\n",
    "    :param user:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    helper = Model_helper(username=user)\n",
    "    colors = helper.preference_maker(2)\n",
    "    y_true = np.array(y_true).reshape(-1)\n",
    "    y_probas = np.array(y_probas).reshape(-1)\n",
    "    probas = y_probas\n",
    "    thresholds, pct1, pct2, ks_statistic, max_distance_at, classes = binary_ks_curve(y_true, probas.ravel())\n",
    "\n",
    "    fig = go.Figure()\n",
    "    idx = np.where(thresholds == max_distance_at)[0][0]\n",
    "    fig.add_shape(dict(type=\"line\", x0=max_distance_at, y0=pct1[idx], x1=max_distance_at, y1=pct2[idx],\n",
    "                       line=dict(color=\"rgba(0,0,0,0)\", width=3, dash=\"dot\")))\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=pct2, mode='lines', name='Class 1', line=dict(color=colors[0])))\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=pct1, mode='lines', name='Class 0', line=dict(color=colors[1])))\n",
    "    fig.add_annotation(x=1, y=0.05, showarrow=False,\n",
    "                       text='KS Statistic : {:.3f} at {:.3f}'.format(ks_statistic, max_distance_at))\n",
    "    fig.update_layout(title_text=f\"<b>KS STATISTICS<b>\", title_x=0.5)\n",
    "    fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig.update_xaxes(title_text='Threshold')\n",
    "    fig.update_yaxes(title_text='Percentage Below Threshold')\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.23, xanchor=\"right\", x=0.6))\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee2aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def confusionmatrix_OngoingComparison(cnf_all, cnf_all2):\n",
    "    \"\"\"\n",
    "\n",
    "    :param cnf_all:\n",
    "    :param cnf_all2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x = ['Positive', 'Negative']\n",
    "    y = ['Positive', 'Negative']\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for step in range(9):\n",
    "        fig.add_trace(go.Heatmap(visible=False, x=x, y=y, z=cnf_all[step], colorscale=\"teal\"))\n",
    "    fig.data[4].visible = True\n",
    "\n",
    "    anno = []\n",
    "    for i, row in enumerate(cnf_all[4]):\n",
    "        for j, value in enumerate(row):\n",
    "            anno.append(\n",
    "                {\n",
    "                    \"x\": x[j],\n",
    "                    \"y\": y[i],\n",
    "                    \"font\": {\"color\": \"white\", \"size\": 16},\n",
    "                    \"text\": str(value),\n",
    "                    \"xref\": \"x1\",\n",
    "                    \"yref\": \"y1\",\n",
    "                    \"showarrow\": False\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create and add slider\n",
    "    steps = []\n",
    "    for i in range(len(fig.data)):\n",
    "        step = dict(method=\"update\", args=[{\"visible\": [False] * len(fig.data)},\n",
    "                                           {\"title\": \"Slider switched to Threshold: \" + str((i + 1) / 10)}],\n",
    "                    label=\"Threshold : \" + str((i + 1) / 10),  # layout attribute\n",
    "                    )\n",
    "        step[\"args\"][0][\"visible\"][i] = True  # Toggle trace to \"visible\"\n",
    "        steps.append(step)\n",
    "    fig.update_layout(title_text=f\"Monitoring Confusion Matrix\", title_x=0.5, annotations=anno)\n",
    "\n",
    "    fig2 = go.Figure()\n",
    "    for step in range(9):\n",
    "        fig2.add_trace(go.Heatmap(visible=False, x=x, y=y, z=cnf_all2[step], colorscale=\"teal\"))\n",
    "    fig2.data[4].visible = True\n",
    "\n",
    "    anno2 = []\n",
    "    for i, row in enumerate(cnf_all2[4]):\n",
    "        for j, value2 in enumerate(row):\n",
    "            anno2.append(\n",
    "                {\n",
    "                    \"x\": x[j],\n",
    "                    \"y\": y[i],\n",
    "                    \"font\": {\"color\": \"white\", \"size\": 16},\n",
    "                    \"text\": str(value2),\n",
    "                    \"xref\": \"x1\",\n",
    "                    \"yref\": \"y1\",\n",
    "                    \"showarrow\": False\n",
    "                }\n",
    "            )\n",
    "    # Create and add slider\n",
    "    steps = []\n",
    "    for i in range(len(fig2.data)):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": [False] * len(fig2.data)},\n",
    "                  {\"title\": \"Slider switched to Threshold: \" + str((i + 1) / 10)}],\n",
    "            label=\"Threshold : \" + str((i + 1) / 10),  # layout attribute\n",
    "        )\n",
    "        step[\"args\"][0][\"visible\"][i] = True  # Toggle trace to \"visible\"\n",
    "        steps.append(step)\n",
    "    fig2.update_layout(title_text=f\"Development Confusion Matrix\", title_x=0.5, annotations=anno2)\n",
    "    \"\"\"WHY??\"\"\"\n",
    "    # confusion_matrix = cnf_all2[4]\n",
    "    # TP = confusion_matrix[1, 1]\n",
    "    # TN = confusion_matrix[0, 0]\n",
    "    # FP = confusion_matrix[0, 1]\n",
    "    # FN = confusion_matrix[1, 0]\n",
    "    # a = cnf_all2[4].shape\n",
    "    # # Calculating false positives\n",
    "    # corrPred = 0\n",
    "    # falsePred = 0\n",
    "    # for row in range(a[0]):\n",
    "    #     for c in range(a[1]):\n",
    "    #         if row == c:\n",
    "    #             corrPred += confusion_matrix[row, c]\n",
    "    #         else:\n",
    "    #             falsePred += confusion_matrix[row, c]\n",
    "    #\n",
    "    return fig, fig2\n",
    "\n",
    "\n",
    "def CM_OngoingComparison(y_pred, y_test, x_test, x_pred):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_pred:\n",
    "    :param y_test:\n",
    "    :param x_test:\n",
    "    :param x_pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    step = np.arange(0.1, 1, 0.1)\n",
    "    cnf_all1 = []\n",
    "    cnf_all2 = []\n",
    "    y_pred = pd.Series(y_pred)\n",
    "    for i in step:\n",
    "        y_pred[y_pred > i] = 1\n",
    "        y_pred[y_pred <= i] = 0\n",
    "        cnf1 = confusion_matrix(y_test, y_pred)\n",
    "        cnf2 = confusion_matrix(x_test, x_pred)\n",
    "        cnf_all1.append(cnf1)\n",
    "        cnf_all2.append(cnf2)\n",
    "    figs = confusionmatrix_OngoingComparison(cnf_all1, cnf_all2)\n",
    "    return figs\n",
    "\n",
    "\n",
    "def confusionmatrix_ModelComparison(cnf_all):\n",
    "    \"\"\"\n",
    "\n",
    "    :param cnf_all:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x = ['Positive', 'Negative']\n",
    "    y = ['Positive', 'Negative']\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for step in range(9):\n",
    "        fig.add_trace(go.Heatmap(visible=False, x=x, y=y, z=cnf_all[step], colorscale=\"teal\"))\n",
    "    fig.data[4].visible = True\n",
    "\n",
    "    anno2 = []\n",
    "    for i, row in enumerate(cnf_all[4]):\n",
    "        for j, value2 in enumerate(row):\n",
    "            anno2.append(\n",
    "                {\n",
    "                    \"x\": x[j],\n",
    "                    \"y\": y[i],\n",
    "                    \"font\": {\"color\": \"white\", \"size\": 16},\n",
    "                    \"text\": str(value2),\n",
    "                    \"xref\": \"x1\",\n",
    "                    \"yref\": \"y1\",\n",
    "                    \"showarrow\": False\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create and add slider\n",
    "    steps = []\n",
    "    for i in range(len(fig.data)):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": [False] * len(fig.data)},\n",
    "                  {\"title\": \"Slider switched to Threshold: \" + str((i + 1) / 10)}],\n",
    "            label=\"Threshold : \" + str((i + 1) / 10),  # layout attribute\n",
    "        )\n",
    "        step[\"args\"][0][\"visible\"][i] = True  # Toggle trace to \"visible\"\n",
    "        steps.append(step)\n",
    "    fig.update_layout(title_text=f\"<b>CONFUSION MATRIX<b>\", title_x=0.5, annotations=anno2)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def CM_ModelComparison(y_pred, y_actual):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_pred:\n",
    "    :param y_actual:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    step = np.arange(0.1, 1, 0.1)\n",
    "    cnf_all = []\n",
    "    y_pred = pd.Series(y_pred)\n",
    "    for i in step:\n",
    "        y_pred[y_pred > i] = 1\n",
    "        y_pred[y_pred <= i] = 0\n",
    "        cnf = confusion_matrix(y_actual, y_pred)\n",
    "        cnf_all.append(cnf)\n",
    "    fig = confusionmatrix_ModelComparison(cnf_all)\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f782d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_sen_OngoingComparison(train_model, test_model, user):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_model:\n",
    "    :param test_model:\n",
    "    :param user:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    helper = Model_helper(username=user)\n",
    "    colors = helper.preference_maker(2)\n",
    "    # ROC for train dataset\n",
    "    model_fpr, model_tpr = train_model.roc.select(\"FPR\").show(), train_model.roc.select(\"TPR\").show()\n",
    "\n",
    "    go.Figure()\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    fig.add_trace(go.Scatter(x=model_fpr, y=1 - model_tpr,\n",
    "                             mode='lines',\n",
    "                             name='Specificity',\n",
    "                             line=dict(color=colors[0]),\n",
    "                             ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=model_fpr, y=model_tpr, mode='lines', name='Sensitivity', line=dict(color=colors[1])),\n",
    "                  secondary_y=True, )\n",
    "    fig.update_layout(title_text=f\"Development<br>Specificity vs Sensitivity\", title_x=0.5)\n",
    "    fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig.update_yaxes(title_text='Specificity')\n",
    "    fig.update_yaxes(title_text='Sensitivity')\n",
    "    fig.update_xaxes(title_text='Cutoff')\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.23, xanchor=\"right\", x=0.6))\n",
    "\n",
    "    # ROC for test dataset\n",
    "    model_fpr, model_tpr = test_model.roc.select(\"FPR\").show(), test_model.roc.select(\"TPR\").show()\n",
    "\n",
    "    go.Figure()\n",
    "    fig2 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    fig2.add_trace(go.Scatter(x=model_fpr, y=1 - model_tpr, mode='lines',\n",
    "                              name='Specificity',\n",
    "                              line=dict(color=colors[0]),\n",
    "                              ))\n",
    "    fig2.add_trace(go.Scatter(x=model_fpr, y=model_tpr, mode='lines', name='Sensitivity', line=dict(color=colors[1])),\n",
    "                   secondary_y=True, )\n",
    "    fig2.update_layout(title_text=f\"Monitoring<br>Specificity vs Sensitivity\", title_x=0.5)\n",
    "    fig2.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig2.update_yaxes(title_text='Specificity')\n",
    "    fig2.update_yaxes(title_text='Sensitivity')\n",
    "    fig2.update_xaxes(title_text='Cutoff')\n",
    "    fig2.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig2.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig2.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.23, xanchor=\"right\", x=0.6))\n",
    "    return fig, fig2\n",
    "\n",
    "\n",
    "def graph_sen_ModelComparison(test_model, user):\n",
    "    \"\"\"\n",
    "\n",
    "    :param test_model:\n",
    "    :param user:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    helper = Model_helper(username=user)\n",
    "    colors = helper.preference_maker(2)\n",
    "\n",
    "    # ROC for test dataset\n",
    "    model_fpr, model_tpr = test_model.roc.select(\"FPR\").show(), test_model.roc.select(\"TPR\").show()\n",
    "\n",
    "    go.Figure()\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=model_fpr, y=1 - model_tpr, mode='lines', name='Specificity', line=dict(color=colors[0])))\n",
    "    fig.add_trace(go.Scatter(x=model_fpr, y=model_tpr, mode='lines', name='Sensitivity', line=dict(color=colors[1]), ),\n",
    "                  secondary_y=True)\n",
    "    fig.update_layout(title_text=f\"<b>SPECIFICITY VS SENSITIVITY<b>\", title_x=0.5)\n",
    "    fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig.update_yaxes(title_text='Specificity ')\n",
    "    fig.update_yaxes(title_text='Sensitivity')\n",
    "    fig.update_xaxes(title_text='Cutoff')\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.23, xanchor=\"right\", x=0.6))\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c760ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strengthOfModel(model, strength_test, target_column):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model: It's train model object\n",
    "    :param strength_test: It's a list of metrics name\n",
    "    :param target_column: Target Variable\n",
    "    :return: It's return dict object which basically contian metrics value for testing data\n",
    "    \"\"\"\n",
    "\n",
    "    test_data = {}\n",
    "    y_actual = model.select(target_column).show()\n",
    "    y_pred = model.select(\"prediction\").show()\n",
    "    for i in strength_test:\n",
    "        if i == 'KS':\n",
    "            test_data['KS'] = KS_I(np.array(y_actual).reshape(-1), y_pred)\n",
    "        elif i == 'gini':\n",
    "            test_data['GINI'] = Gini(np.array(y_actual).reshape(-1), y_pred)\n",
    "        elif i == 'ROC-AUC':\n",
    "            test_data['ROC Score'], test_data['AUC Score'] = model.areaUnderROC, model.roc.show()\n",
    "        elif i == 'specificity & sensitivity':\n",
    "            test_data['Sensitivity'] = model.truePositiveRateByLabel\n",
    "            test_data['Specificity'] = model.falsePositiveRateByLabel\n",
    "        elif i == 'Precision':\n",
    "            test_data['Precision'] = model.precisionByLabel\n",
    "        elif i == 'Recall':\n",
    "            test_data['Recall'] = model.recallByLabel\n",
    "        elif i == 'F1-Score':\n",
    "            test_data['F1-Score'] = model.fMeasure()\n",
    "        elif i == \"Accuracy\":\n",
    "            test_data['Accuracy'] = model.accuracy\n",
    "    return test_data\n",
    "\n",
    "\n",
    "def draw_table_OngoingComparison(train_model, test_model, strength_test, target_column):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_model: It's model object of training data\n",
    "    :param test_model: It's model object of testing data\n",
    "    :param strength_test: It's a list of metrics name\n",
    "    :param target_column: Target Variable\n",
    "    :return: t's return dict object which basically contian metrics value for training and testing data\n",
    "    \"\"\"\n",
    "\n",
    "    train_data = {}\n",
    "    y_actual = train_model.select(target_column).show()\n",
    "    y_pred = train_model.select(\"prediction\").show()\n",
    "    for i in strength_test:\n",
    "        if i == 'KS':\n",
    "            train_data['KS'] = KS_I(np.array(y_actual).reshape(-1), y_pred)\n",
    "        elif i == 'gini':\n",
    "            train_data['GINI'] = Gini(np.array(y_actual).reshape(-1), y_pred)\n",
    "        elif i == 'ROC-AUC':\n",
    "            train_data['ROC Score'], train_data[\n",
    "                'AUC Score'] = train_model.summary.areaUnderROC, train_model.summary.roc.show()\n",
    "        elif i == 'specificity & sensitivity':\n",
    "            train_data['Sensitivity'] = train_model.summary.truePositiveRateByLabel\n",
    "            train_data['Specificity'] = train_model.summary.falsePositiveRateByLabel\n",
    "        elif i == 'Precision':\n",
    "            train_data['Precision'] = train_model.summary.precisionByLabel\n",
    "        elif i == 'Recall':\n",
    "            train_data['Recall'] = train_model.summary.recallByLabel\n",
    "        elif i == 'F1-Score':\n",
    "            train_data['F1-Score'] = train_model.summary.fMeasure()\n",
    "        elif i == \"Accuracy\":\n",
    "            train_data['Accuracy'] = train_model.summary.accuracy\n",
    "\n",
    "    test_data = strengthOfModel(test_model, strength_test, target_column)\n",
    "\n",
    "    result = {'Strength': list(test_data.keys()), 'Monitoring': list(test_data.values()),\n",
    "              'Development': list(train_data.values())}\n",
    "    result = pd.DataFrame(result).round(2)\n",
    "    return result\n",
    "\n",
    "\n",
    "def draw_table_ModelComparison(model, strength_test, target_column):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model:\n",
    "    :param strength_test:\n",
    "    :param target_column:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tbl = {}\n",
    "    test_data = strengthOfModel(model, strength_test, target_column)\n",
    "    tbl[\"Strength\"] = list(test_data.keys())\n",
    "    tbl[\"Score\"] = list(test_data.values())\n",
    "    tbl = pd.DataFrame(tbl).round(2)\n",
    "    return tbl\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f38ef838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lift_OngoingComparison(ann_fpr, ann_tpr, ns_fpr, ns_tpr, ann_auc, ann_fpr2, ann_tpr2, ns_fpr2, ns_tpr2, ann_auc2,\n",
    "                           name, user):\n",
    "    \"\"\"\n",
    "\n",
    "    :param ann_fpr:\n",
    "    :param ann_tpr:\n",
    "    :param ns_fpr:\n",
    "    :param ns_tpr:\n",
    "    :param ann_auc:\n",
    "    :param ann_fpr2:\n",
    "    :param ann_tpr2:\n",
    "    :param ns_fpr2:\n",
    "    :param ns_tpr2:\n",
    "    :param ann_auc2:\n",
    "    :param name:\n",
    "    :param user:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    helper = Model_helper(username=user)\n",
    "    colors = helper.preference_maker(2)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=ann_fpr, y=ann_tpr, mode='lines', name=f'{name}', line=dict(color=colors[0])))\n",
    "    fig.add_trace(go.Scatter(x=ns_fpr, y=ns_tpr, mode='lines', name='No Skill', ine=dict(color=colors[1])))\n",
    "    fig.add_annotation(x=max(ns_fpr), y=0.05, showarrow=False, text=\"AUC : \" + str(ann_auc))\n",
    "    fig.update_layout(title_text=f\"Monitoring ROC AUC Curve\", title_x=0.5)\n",
    "    fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig.update_xaxes(title_text='False Positive Rate')\n",
    "    fig.update_yaxes(title_text='True Positive Rate')\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.23, xanchor=\"right\", x=0.6))\n",
    "\n",
    "    fig2 = go.Figure()\n",
    "    fig2.add_trace(go.Scatter(x=ann_fpr2, y=ann_tpr2, mode='lines', name=f'{name}', line=dict(color=colors[0])))\n",
    "    fig2.add_trace(go.Scatter(x=ns_fpr2, y=ns_tpr2, mode='lines', name='No Skill', line=dict(color=colors[1])))\n",
    "    fig2.add_annotation(x=max(ns_fpr2), y=0.05, showarrow=False, text=\"AUC : \" + str(ann_auc2))\n",
    "    fig2.update_layout(title_text=f\"Development ROC AUC Curve\", title_x=0.5)\n",
    "    fig2.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig2.update_xaxes(title_text='False Positive Rate')\n",
    "    fig2.update_yaxes(title_text='True Positive Rate')\n",
    "    fig2.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig2.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig2.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.23, xanchor=\"right\", x=0.6))\n",
    "    return fig, fig2\n",
    "\n",
    "\n",
    "# ROC AUC function for ModelComparison\n",
    "def lift_ModelComparison(ann_fpr, ann_tpr, ns_fpr, ns_tpr, ann_auc, name, user):\n",
    "    \"\"\"\n",
    "\n",
    "    :param ann_fpr:\n",
    "    :param ann_tpr:\n",
    "    :param ns_fpr:\n",
    "    :param ns_tpr:\n",
    "    :param ann_auc:\n",
    "    :param name:\n",
    "    :param user:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    helper = Model_helper(username=user)\n",
    "    colors = helper.preference_maker(2)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=ann_fpr, y=ann_tpr, mode='lines', name=f'{name}', line=dict(color=colors[0])))\n",
    "    fig.add_trace(go.Scatter(x=ns_fpr, y=ns_tpr, mode='lines', name='No Skill', ine=dict(color=colors[1])))\n",
    "    fig.add_annotation(x=max(ns_fpr), y=0.05, showarrow=False, text=\"AUC : \" + str(ann_auc))\n",
    "    fig.update_layout(title_text=f\"<b>ROC AUC Curve<b>\", title_x=0.5)\n",
    "    fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig.update_xaxes(title_text='False Positive Rate')\n",
    "    fig.update_yaxes(title_text='True Positive Rate')\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.23, xanchor=\"right\", x=0.6))\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ded8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_AUC_ModelComparison(model, name, target_column, user):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model:\n",
    "    :param name:\n",
    "    :param target_column:\n",
    "    :param user:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model_auc = model.areaUnderROC\n",
    "    model_fpr, model_tpr = model.roc.select(\"FPR\").show(), model.roc.select(\"TPR\").show()\n",
    "    ns_probs = [0 for _ in range(len(model.select(\"prediction\").count()))]\n",
    "    train_actual = model.select(target_column)\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(train_actual, ns_probs)\n",
    "    return lift_ModelComparison(model_fpr, model_tpr, ns_fpr, ns_tpr, model_auc, name, user)\n",
    "\n",
    "\n",
    "# ROC AUC Curve\n",
    "def draw_AUC_OngoingComparison(train_model, test_model, target_column, name, user):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_model:\n",
    "    :param test_model:\n",
    "    :param target_column:\n",
    "    :param name:\n",
    "    :param user:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # ROC for Train Data\n",
    "    train_model_auc = train_model.areaUnderROC\n",
    "    train_model_fpr, train_model_tpr = train_model.roc.select(\"FPR\").show(), train_model.roc.select(\"TPR\").show()\n",
    "    ns_probs1 = [0 for _ in range(len(train_model.select(\"prediction\").count()))]\n",
    "    train_actual = train_model.select(target_column)\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(train_actual, ns_probs1)\n",
    "\n",
    "    # ROC for Test Data\n",
    "    test_model_auc = test_model.areaUnderROC\n",
    "    test_model_fpr, test_model_tpr = test_model.roc.select(\"FPR\").show(), test_model.roc.select(\"TPR\").show()\n",
    "    ns_probs2 = [0 for _ in range(len(test_model.select(\"prediction\").count()))]\n",
    "    test_actual = test_model.select(target_column)\n",
    "    ns_fpr2, ns_tpr2, _ = roc_curve(test_actual, ns_probs2)\n",
    "\n",
    "    return lift_OngoingComparison(test_model_fpr, test_model_tpr, ns_fpr, ns_tpr, train_model_auc, train_model_fpr,\n",
    "                                  train_model_tpr, ns_fpr2,\n",
    "                                  ns_tpr2, test_model_auc, name, user=user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7315d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"VIF Test\"\"\"\n",
    "\n",
    "\n",
    "def calc_vif_OngoingComparison(x_test, y_test):\n",
    "    \"\"\"\n",
    "\n",
    "    :param x_test:\n",
    "    :param y_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vif = {\"Variable\": x_test.columns.tolist(),\n",
    "           \"VIF Train\": [variance_inflation_factor(x_test.values, i) for i in range(x_test.shape[1])],\n",
    "           \"VIF Test\": [variance_inflation_factor(y_test.values, i) for i in range(y_test.shape[1])]}\n",
    "    vif = pd.DataFrame(vif).round(2)\n",
    "    return vif\n",
    "\n",
    "\n",
    "def calc_vif_ModelComparison(X):\n",
    "    \"\"\"\n",
    "\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vif = {\"Variable\": X.columns.tolist(), \"VIF\": [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]}\n",
    "    vif = pd.DataFrame(vif).round(2)\n",
    "    return vif\n",
    "\n",
    "\n",
    "# HL test\n",
    "def make_recarray(y_true, y_pred):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    recarrays = np.recarray((len(y_true),), [('y_true', 'u8'), ('y_pred', 'f8')])\n",
    "    recarrays['y_true'] = y_true\n",
    "    recarrays['y_pred'] = y_pred\n",
    "    recarrays.sort(order='y_pred')\n",
    "    return recarrays\n",
    "\n",
    "\n",
    "def hosmer_lemeshow_table(y_true, y_pred, n_groups=10):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :param n_groups:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if n_groups < 2:\n",
    "        raise ValueError('Number of groups must be greater than or equal to 2')\n",
    "    elif n_groups > len(y_true):\n",
    "        raise ValueError('Number of predictions must exceed number of groups')\n",
    "    table = make_recarray(y_true, y_pred)\n",
    "    table = [(len(g), g.y_true.sum(), g.y_pred.sum(), g.y_pred.mean()) for g in np.array_split(table, n_groups)]\n",
    "    names = ('group_size', 'obs_freq', 'pred_freq', 'mean_prob')\n",
    "    table = np.rec.fromrecords(table, names=names)\n",
    "    return table\n",
    "\n",
    "\n",
    "def hlTest(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_true:\n",
    "    :param y_prob:\n",
    "    :param model_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    table = hosmer_lemeshow_table(y_true, y_prob)\n",
    "    num = np.square(table.obs_freq - table.mean_prob)\n",
    "    den = table.group_size * table.mean_prob * (1 - table.mean_prob)\n",
    "    C_hat = np.sum(num / den)\n",
    "    df = len(table) - 2\n",
    "    p = scipy.stats.distributions.chi2.sf(C_hat, df)\n",
    "\n",
    "    if p > 0.05:\n",
    "        conclusion = 'The Model is adequate'\n",
    "    else:\n",
    "        conclusion = 'The Model is not adequate'\n",
    "\n",
    "    table = {'Groups': 10, 'HL test Statistic': C_hat, 'DOF': df, 'P-Value': p, 'Conclusion': conclusion}\n",
    "    table = pd.DataFrame(table, index=[model_name])\n",
    "    table = table.round(2)\n",
    "    # TestResult = namedtuple('HosmerLemeshowTest', ('C_hat', 'df', 'p'))\n",
    "    return table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83bbe07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_importance_graph_ModelComparison(target_column, test_data):\n",
    "    \"\"\"\n",
    "\n",
    "    :param target_column:\n",
    "    :param test_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    features = test_data.drop(target_column).show()\n",
    "    rf_model = RandomForestClassifier(labelCol=target_column, featuresCol=features, numTrees=10)\n",
    "    rf_model = rf_model.fit(test_data)\n",
    "    fi = pd.DataFrame({\"Features\": features, \"Importance\": [i for i in rf_model.featureImportances.values]})\n",
    "    x, y = (list(x) for x in zip(*sorted(zip(rf_model.featureImportances.values, features), reverse=True)))\n",
    "    trace = go.Bar(x=y, y=x, marker=dict(color='#32E0C4'))\n",
    "    data = [trace]\n",
    "    layout = go.Layout()\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig.update_layout(title_text=\"<b>RANDOM FOREST BASED FEATURE IMPORTANCE<b> \", title_x=0.5)\n",
    "    fig.update_xaxes(title_text='Features')\n",
    "    fig.update_yaxes(title_text='Feature Importance')\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    return fi.round(2), fig\n",
    "\n",
    "\n",
    "def feature_importance_graph_OngoingComparison(target_column, dev, mon):\n",
    "    features = mon.drop(target_column)\n",
    "    rf_model = RandomForestClassifier(labelCol=target_column, featuresCol=features, numTrees=10)\n",
    "    rf_model = rf_model.fit(mon)\n",
    "\n",
    "    fi = pd.DataFrame({\"Features\": features, \"Importance\": rf_model.featureImportances.values})\n",
    "    x, y = (list(x) for x in zip(*sorted(zip(rf_model.featureImportances.values, features), reverse=True)))\n",
    "    trace = go.Bar(x=y, y=x, marker=dict(color='#32E0C4'))\n",
    "    data = [trace]\n",
    "    layout = go.Layout()\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig.update_layout(title_text=\"Monitoring Sample<br>Random Forest Based Feature Importances\", title_x=0.5)\n",
    "    fig.update_xaxes(title_text='Features')\n",
    "    fig.update_yaxes(title_text='Feature Importance')\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "\n",
    "    rf_model = rf_model.fit(dev)\n",
    "    fi['Importance Development Sample'] = rf_model.featureImportances.values\n",
    "    x, y = (list(x) for x in zip(*sorted(zip(rf_model.feature_importances_, features), reverse=True)))\n",
    "    trace = go.Bar(x=y, y=x, marker=dict(color='#32E0C4'))\n",
    "    data = [trace]\n",
    "    layout = go.Layout()\n",
    "    fig2 = go.Figure(data=data, layout=layout)\n",
    "    fig2.update_layout({'plot_bgcolor': 'rgba(0,0,0,0)', 'paper_bgcolor': 'rgba(0,0,0,0)'})\n",
    "    fig2.update_layout(title_text=\"Development Sample<br>Random Forest Based Feature Importance\", title_x=0.5)\n",
    "    fig2.update_xaxes(title_text='Features')\n",
    "    fig2.update_yaxes(title_text='Feature Importance')\n",
    "    fig2.update_xaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    fig2.update_yaxes(showline=True, linewidth=1, linecolor='black', rangemode='nonnegative')\n",
    "    return fi.round(2), (fig, fig2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "091e467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ModelComparison(user, models, target_column, dataset, stabilities, strengths, model_names):\n",
    "    \"\"\"\n",
    "\n",
    "    :param user:\n",
    "    :param models:\n",
    "    :param target_column:\n",
    "    :param dataset: All data point of dataset\n",
    "    :param stabilities:\n",
    "    :param strengths:\n",
    "    :param model_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    figs = {}\n",
    "    tables = {}\n",
    "    for i in range(len(models)):\n",
    "        if model_names[i] is None:\n",
    "            model_name = \"Model \" + str(i + 1)\n",
    "        else:\n",
    "            model_name = model_names[i].capitalize()\n",
    "        model = models[i]\n",
    "        figs[model_name] = {}\n",
    "        tables[model_name] = {}\n",
    "\n",
    "        train_data, test_data = dataset.randomSplit([.7, .3], seed=42)\n",
    "        train_model = model.transform(train_data)\n",
    "        train_model = model.evaluate(train_data).select(\"prediction\").show()\n",
    "        test_model = model.evaluate(test_data).select(\"prediction\").show()\n",
    "        y_pred = model.evaluate(dataset).select(\"prediction\").show()  # prediction classes for all dataset\n",
    "        y_actual = dataset.select(target_column).show()  # actual classes for dataset\n",
    "\n",
    "        for stability in stabilities:\n",
    "            if stability == 'PSI':\n",
    "                tables[model_name][stability] = PSI_ModelComparison(train_model, test_model)\n",
    "            elif stability == 'CSI':\n",
    "                tables[model_name][stability] = calculate_csi(test_data, train_data)\n",
    "        for strength in strengths:\n",
    "            if strength == 'ROC-AUC':\n",
    "                fig = draw_AUC_ModelComparison(model, model_name, target_column, user)\n",
    "                fig.update_layout(title_text=f\"ROC AUC Curve: {model_name}\", )\n",
    "                figs[model_name][strength] = fig\n",
    "            elif strength == 'Confusion Matrix':\n",
    "                fig = CM_ModelComparison(y_pred, y_actual)\n",
    "                fig.update_layout(title_text=f\"Confusion Matrix : {model_name}\", )\n",
    "                figs[model_name][strength] = fig\n",
    "            elif strength == 'KS':\n",
    "                fig = KS_ModelComparison(y_pred, y_actual, user)\n",
    "                fig.update_layout(title_text=f\"KS Statistics Curve : {model_name}\", )\n",
    "                figs[model_name][strength] = fig\n",
    "            elif strength == 'specificity & sensitivity':\n",
    "                fig = graph_sen_ModelComparison(test_model, user)\n",
    "                fig.update_layout(title_text=f\"Specificity vs Sensitivity : {model_name}\", )\n",
    "                figs[model_name][strength] = fig\n",
    "            elif strength == 'vif':\n",
    "                table = calc_vif_ModelComparison(dataset)\n",
    "                tables[model_name][strength] = table\n",
    "            elif strength == 'HL':\n",
    "                table = hlTest(np.array(y_actual).reshape(-1), y_pred, model_name)\n",
    "                tables[model_name][strength] = table\n",
    "        metrics_name = [x for x in strengths if\n",
    "                        x in ['ROC-AUC', 'gini', 'KS', 'specificity & sensitivity', 'Precision', 'Recall', 'F1-Score']]\n",
    "        tables[model_name]['Strength Statistics'] = draw_table_ModelComparison(test_model, metrics_name, target_column)\n",
    "        table, fig = feature_importance_graph_ModelComparison(target_column, dataset)\n",
    "        fig.update_layout(title_text=f\"Random Forest Based Feature Importance : {model_name}\", )\n",
    "        tables[model_name]['Feature Importance'] = table\n",
    "        figs[model_name]['Feature Importance'] = fig\n",
    "    return figs, tables\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f54b5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wrapper for Ongoing Comparison\n",
    "def OngoingComparison(user, models, target_column, test_data, train_data, stabilities, strengths, model_names):\n",
    "    figs = {}\n",
    "    tables = {}\n",
    "    for i in range(len(models)):\n",
    "        if model_names[i] is None:\n",
    "            model_name = \"Model \" + str(i + 1)\n",
    "        else:\n",
    "            model_name = model_names[i].capitalize()\n",
    "        model = models[i]\n",
    "        figs[model_name] = {}\n",
    "        tables[model_name] = {}\n",
    "\n",
    "        train_model = model.evaluate(train_data)  # probability of both the classes for train data\n",
    "        test_model = model.evaluate(test_data)\n",
    "        train_data, test_data = train_data.drop(target_column), test_data.drop(target_column)\n",
    "\n",
    "        for stability in stabilities:\n",
    "            if stability == 'PSI':\n",
    "                tables[test_data][stability] = PSI_OngoingComparison(train_model.select(\"prediction\"),\n",
    "                                                                     test_model.select(\"prediction\"))\n",
    "            elif stability == 'CSI':\n",
    "                tables[test_data][stability] = calculate_csi(test_data, train_data)\n",
    "        for strength in strengths:\n",
    "            if strength == 'ROC-AUC':\n",
    "                figs[test_data][strength] = draw_AUC_OngoingComparison(train_model, test_model, target_column,\n",
    "                                                                       model_name,\n",
    "                                                                       user)\n",
    "            elif strength == 'Confusion Matrix':\n",
    "                figs[test_data][strength] = CM_OngoingComparison(y_pred=test_model,\n",
    "                                                                 y_test=test_data.select(target_column),\n",
    "                                                                 x_test=test_model,\n",
    "                                                                 x_pred=train_data.select(target_column))\n",
    "            elif strength == 'KS':\n",
    "                figs[test_data][strength] = KS_OngoingComparison(user=user, y_probas=train_model.select(\"prediction\"),\n",
    "                                                                 y_true=train_data.select(target_column),\n",
    "                                                                 x_probas=test_model.select(\"prediction\"),\n",
    "                                                                 x_true=test_data.select(target_column))\n",
    "            elif strength == 'specificity & sensitivity':\n",
    "                figs[test_data][strength] = graph_sen_OngoingComparison(train_model, test_model, user=user)\n",
    "            elif strength == 'vif':\n",
    "                table = calc_vif_OngoingComparison(train_data, test_data)\n",
    "                table.rename(columns={'VIF Test': 'VIF Monitoring', 'VIF Train': 'VIF Development'}, inplace=True)\n",
    "                tables[test_data][strength] = table\n",
    "            elif strength == 'HL':\n",
    "                table = hlTest(np.array(test_data.select(target_column)).reshape(-1),\n",
    "                               test_model.select(\"prediction\").show(), model_name='model')\n",
    "                tables[test_data][strength] = table\n",
    "\n",
    "        metrics_list = [x for x in strengths if x in ['ROC-AUC', 'gini', 'KS', 'specificity & sensitivity',\n",
    "                                                      'Precision', 'Recall', 'F1-Score']]\n",
    "        tables[test_data]['Strength Statistics'] = draw_table_OngoingComparison(train_model, test_model,\n",
    "                                                                                metrics_list, target_column)\n",
    "        table, fig = feature_importance_graph_OngoingComparison(target_column, train_data, test_data)\n",
    "        tables[test_data]['Feature Importance'] = table\n",
    "        figs[test_data]['Feature Importance'] = fig\n",
    "    return figs, tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1dd96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('master': conda)",
   "language": "python",
   "name": "python395jvsc74a57bd0e5d34a57892e62c64696f4f28864301f58fd12f04690d261f74726cd452afbc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
